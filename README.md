# LLM Introspection

This repo contains my summer project exploring interpretability techniques for introspecting the internal representations of LLMs.

## Goals
- Probe and visualize hidden states, attention maps, and internal activations.
- Identify patterns tied to reasoning steps, factual recall, or hallucinations.
- Compare internal dynamics across model sizes and fine-tuning regimes.
- Develop intuitive tools for interpreting token-level model behavior.

## Structure
- `activations/` 
- `viz/`
- `eval/` 
- `notebooks/`
- `logs/`

## Note
Because it is still in the early stages, the structure and code is subject to change as I learn more throughout the process. 
